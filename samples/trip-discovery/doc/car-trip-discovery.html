<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html><head>
  <meta content="text/html; charset=ISO-8859-1" http-equiv="content-type"><title>car-trip-discovery</title></head><body>
<h1>Vehicle Trip Discovery with GIS Tools for Hadoop</h1>

<p>
A common highway management task is to study potential impact of driver
carpooling, based on
automobile GPS position data. To identify potential carpool
improvements, we set out to study
places that have the highest numbers of trips with
nearly common origin &amp; destination locations.&nbsp; The source data
available consists of
39 million records of vehicle positions in Japan, from data
transmitted by the GPS units of cars in one day.&nbsp;
The position data consists of longitude &amp; latitude in CSV (not GPX
tracks).&nbsp; We used the Hadoop MapReduce framework for parallel
computation, considering its capability for analyzing larger data sets.</p>

The
study area was the country of Japan, selected from World Map in the
data included with ArcGIS. It was projected to the Tokyo Geographic
Coordinate System, with the <em>Project</em> geoprocessing tool in ArcMap, to
match the spatial reference of the GPS data.<br>

<p><img style="width: 413px; height: 333px;" alt="[GP Tool: Project to Tokyo GCS]" src="project-japan-to-tgcs.png"><br>
</p>

<br>

<p>Next, the study area was exported to JSON format, suitable for use
in computations on Hadoop, by using the <em>Features To JSON</em> tool.</p>

<p><img style="width: 425px; height: 247px;" alt="[GP Tool: Features To JSON]" src="features-json-japan-arcmap.png"><br>
</p>

<p>The input feature class is the country of Japan in Tokyo GCS, as
produced in the previous step.&nbsp; The output JSON parameter is a
filename of our choosing, with <code>.json</code> extension.&nbsp; For
the remaining parameters, the defaults are suitable: JSON type (<span style="font-size: smaller;">ENCLOSED_JSON</span>) and Formatted JSON
(unchecked).<br>
</p>


<p>Then the JSON file representing the features, was copied to HDFS, to
be accessible to the computations on Hadoop.&nbsp; This can be done
either with command-line <code>hadoop fs</code>, or by using the <em>Copy To HDFS</em> tool.</p>

<p><img style="width: 441px; height: 315px;" alt="[GP Tool: Copy To HDFS]" src="copy-to-hdfs-japan-arcmap.png"><br>
</p>

<p>The tool expects the following parameters: </p>

<ul>

  <li>The Input local file, is the JSON file output by the previous
step.</li>
  <li>The
HDFS parameters for hostname, port, &amp; username - these relate to
the configuration of Hadoop; values can be provided by the system
administrator (the default port number should work unless the admin has
configured a non-default port).</li>
  <li>The HDFS remote file - the path &amp; file name where the tool
will copy the file.<br>
  </li>
</ul>


<p>Calculations were done in two stages of MapReduce applications on
Hadoop. As a cursory overview of MapReduce:&nbsp; first a Mapper
associates a list of keys each with a list of values, then for each
key, a Reducer performs a calculation on the values for that key, and
emits any output records for that key. The
first MapReduce application used in our study, creates a grid of cells
covering Japan,
infers origin &amp; destination of trips from sequences of vehicle
positions, and identifies the grid cell containing the origin &amp;
destination point of each inferred trip.<br>
</p>

<p>Columns of the CSV file of GPS positions, include vehicle ID, date,
time, position as longitude &amp; latitude (degrees-minutes-seconds),
and speed (km/h). The mapper of the first-stage MapReduce application,
reads the input CSV data, and treats the combination of car-ID and date
as the key - and the remainder of the input fields, as the value
associated with such key.&nbsp; Thus, the data is grouped by car and
date, for passing to the reducer, a separate list of position
records&nbsp;for each car &amp; date.<br>
</p>

<iframe src="TripCellMapper.txt"
 alt="[Source code: mapper on raw data]"
 style="width:100%;height:36em"></iframe>

<p>In order to support trips that span midnight - when in possession of
multiple days of data - a mapper could use a key consisting of the car ID only -
passing to the reducer, a potentially longer list of position records,
which would need to be sorted by date &amp; time.<br>
</p>


<p>The grid of cells to cover Japan, is calculated in the setup of the
reducer of the first MapReduce application - once per reducer
node.&nbsp; If we had simply used an equal-angle grid with the
geographic coordinates, the geodesic area of a cell would have differed
by about 18% between northernmost &amp; southernmost Japan - so we
generate an equal-area grid, while still using geographic coordinates.
To
produce an equal-area grid, the code takes the envelope of the study
area, and uses the middle latitude as a baseline.&nbsp; It uses
<code>GeometryEngine.geodesicDistanceOnWGS84</code> to calculate the
length of a 1°
arc along the latitude (using WGS84 to make a close approximation of
distance - not position - on Tokyo GCS), then uses proportions to find
the X-axis angle corresponding to the desired cell width.&nbsp; With a
constant X-axis angle, it calculates variable Y-axis grid angles,
starting at the southernmost latitude and working northward, using
<code>GeometryEngine.geodesicDistanceOnWGS84</code> on the X-axis angle
cell width,
and dividing into the constant area to get the Y-axis angle for each
row of the grid.<br>
</p>

<iframe src="TripCellRed-setup.txt"
 alt="[Source code: buildGrid]"
 style="width:100%;height:93em"></iframe>


<p>Finding the cell containing a location point, when the cells are
stored as an array of bounds, and each row has the same number of
cells, proceeds as follows.&nbsp; The X-axis index is a straightforward
division by cell width.&nbsp; The Y-axis calculation is adjusted for
possible overshoot or undershoot, due to varying cell height.&nbsp;
Then <code>cellIndex = xIndex + xCount * yIndex</code>.<br>
</p>

<iframe src="TripCellRed-queryGrid.txt"
 alt="[Source code: queryGrid]"
 style="width:100%;height:30em"></iframe>


<p>Inferred trips were calculated as follows.&nbsp; The GPS units in
the cars transmit a point of position data about every 30 seconds (with
some variation), while the car is on. For successive positions for the
same car, the code looks for lapses in the position points. For each
lapse of more than 15 minutes, it interprets the position before the
lapse as the destination of a trip, and the position after the lapse as
the origin of a new trip. Also, the last position of the car in the
day, is interpreted as the destination of a trip (except in the case of
a lone position point).<br>
</p>

<iframe src="TripCellRed-reduce.txt"
 alt="[Source code: infer trips]"
 style="width:100%;height:85em"></iframe>


<p>This trip-inference computation relies on the fact that the car GPS
unit does not transmit data when the car is off. For trip discovery on
data from GPS units that continue transmitting while the vehicle is
off, it would be necessarily to additionally check whether the car has
in fact moved more than a threshold roaming distance (using <code>GeometryEngine.geodesicDistanceOnWGS84</code>).</p>

<p><br>
</p>

<p>The input for the second-stage MapReduce job, was the output of the
first stage, namely the longitude, latitude, and grid-cell bounds of
both the origin &amp; destination of the trips. The second-stage
MapReduce job grouped the inferred trips by origin cell, in the mapper.
Then for
each origin cell, the reducer counted trips by grid cell containing the
destination of the trip, to determine - for that origin cell - the
number of trips ending in the most-common destination cell.</p>

<iframe src="TripCorrRed-reduce.txt"
 alt="[Source code: common destination by origin cell]"
 style="width:100%;height:31em"></iframe>


<p>To execute the calculations, the MapReduce jobs can be invoked by
using either the <em>Execute Workflow</em> tool in ArcMap, or the
command
line.&nbsp; Here are recipes for invoking the MapReduce jobs from
command line:</p>

<iframe src="trips-invoke-mapred.txt"
 alt="[Command line: invoke MapReduce jobs]"
 style="width:100%;height:11em"></iframe>


<p>The tab-separated output was converted to JSON - a format suitable
for import to ArcMap - with Hive queries using ST_Geometry
functions from GIS Tools for Hadoop.</p>

<iframe src="trips-result-json.txt"
 alt="[Hive query: convert to JSON from tab-separated]"
 style="width:100%;height:17em"></iframe>


<p>Next, the JSON file of results, was copied from HDFS, to
be accessible to ArcMap.&nbsp; This can be done
either with command-line <code>hadoop</code> <code>fs</code>, or by using the <em>Copy From HDFS</em> tool.</p>
<img style="width: 423px; height: 280px;" alt="[GP Tool: Copy From HDFS]" src="copy-from-hdfs-trips-arcmap.png">
<p>The tool expects the following parameters: </p>


<ul>
<li>The
HDFS parameters for hostname, port, &amp; username - these relate to
the configuration of Hadoop - same as with the Copy To HDFS tool.</li><li>The HDFS remote file - is the directory in HDFS generated by the previous
step - it contains a JSON file.<br></li>
  <li>The Output local file parameter is a
filename of our choosing, with <code>.json</code> extension.<br>
  </li>
</ul>


<p>
</p>


<p>Then the results were imported to ArcMap as a feature class, by using the <em>JSON To Features</em> tool.</p>
<img style="width: 391px; height: 221px;" alt="[GP tool: JSON To Features]" src="json-features-trips-arcmap.png">
<p>The Input JSON is the file copied over in the previous step.&nbsp;
The Output feature class is a new ArcGIS feature class, with a name of
our choosing.&nbsp; For JSON type, be sure to select UNENCLOSED_JSON.</p>
As the unenclosed JSON that was imported does not carry information as
to spatial reference, it is necessary to right click the newly-imported
feature class in the Catalog - then Properties, XY Coordinate System,
Geographic Coordinate Systems, Asia, Tokyo.
<p><img style="width: 515px; height: 469px;" alt="[Feature Class Properties: Spatial Reference]" src="spatial-reference-arcmap.png"><br>
</p>

<p> Finally, we used ArcMap to visualize the results in a map.&nbsp; As
the JSON that was imported had all fields as text (rather than number),
it was necessary to add an integer field for the count of the trips
from each origin cell, that ended in the most common destination cell -
which the ArcMap field calculator populated handily (from the text
field that contained the number as character digits).&nbsp; Then, it
was possible to use this integer-value field for symbols varying by
quantity. The map shows
cells that were the origin of five or more trips to a common
destination cell. Cells that were the origin of 11 or trips to a
common destination, are symbolized with bigger &amp; darker purple
squares, to highlight candidate areas for carpool suggestions.</p>

<img style="width: 640px; height: 495px;" alt="Map: by origin cell, count of car trips to common destination cell" src="cars-jp20b.png"><br>

<p>A
potential further study could additionally consider the following:</p>

<ul>

  <li>The location and distance of the most-common destination cell,
for each origin cell of interest;</li>
  <li>Time slices, as many people need to go to their destination
during a specific part of the day.</li>
</ul>

<p>The following open-source projects, from GIS Tools for Hadoop on
Github, were used:</p>

<ul>

  <li>The <a href="https://github.com/Esri/geometry-api-java">geometry-api-java
library</a> was used for computing equal-area grid cells.</li>
  <li>From <a href="https://github.com/Esri/spatial-framework-for-hadoop">spatial-framework-for-hadoop</a>,
Hive UDFs were used to create
geometries from raw data, along with API utilities to export &amp; import
data to &amp; from
JSON.</li>
  <li>Geoprocessing tools were used from <a href="https://github.com/Esri/geoprocessing-tools-for-hadoop">geoprocessing-tools-for-hadoop</a>,
to import results into ArcMap for visualization, as well as to export a
polygon of Japan for gridding.<br>
  </li>
</ul>

<p>
The MapReduce calculations ran in under an hour on a single-node
development instance of Hadoop. This provides a proof of concept
of using a Hadoop cluster to run similar calculations on much bigger
data sets.</p>
<p>The complete source code is <a href="https://github.com/randallwhitman/gis-tools-for-hadoop">available on Github</a>.<br>
</p>



</body></html>
